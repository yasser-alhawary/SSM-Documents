#
# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this
# software and associated documentation files (the "Software"), to deal in the Software
# without restriction, including without limitation the rights to use, copy, modify,
# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
# permit persons to whom the Software is furnished to do so.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#
---
description: |
  ### Document name - AWS-DeleteEKSCluster

  ## What does this document do?
  * This document deletes the resources associated with an Amazon EKS cluster as in [this link](https://docs.aws.amazon.com/eks/latest/userguide/delete-cluster.html#delete-cluster-cli).
  * Deletes all node groups and Fargate profiles
  * (Optional) Deletes all self-managed node AWS CloudFormation stacks.
  * (Optional) Delete the VPC AWS CloudFormation stack

    ### NOTE
    If you have active services in your cluster that are associated with a load balancer, you must delete those services before deleting the cluster so that the load balancers are deleted properly. Otherwise, you can have orphaned resources in your VPC that prevent you from being able to delete the VPC.

    ## [ To delete an Amazon EKS cluster with AWS-DeleteEKSCluster Automation document]
    1. List all services running in your cluster.

        ```kubectl get svc --all-namespaces```

    2. Delete any services that have an associated EXTERNAL-IP value. These services are fronted by an Elastic Load Balancing load balancer, and you must delete them in Kubernetes to allow the load balancer and associated resources to be properly released.

        ```kubectl delete svc <service-name>```

    3. Execute the AWS-DeleteEKSCluster Automation document.

    ## Input Parameters
    * EKSClusterName: (Required) The name of the Amazon EKS Cluster to be deleted.
    * VPCCloudFormationStack: (Optional) AWS Cloudformation stack name for VPC for the EKS cluster being deleted. This will delete the AWS Cloudformation stack for VPC.
    * VPCCloudFormationStackRole: (Optional) The ARN of an IAM role that AWS CloudFormation assumes to delete the VPC CloudFormation stack. AWS CloudFormation uses the role's credentials to make calls on your behalf.
    * SelfManagedNodeStacks: (Optional) Comma-separated  list of AWS Cloudformation stack names for self-managed nodes, This will delete the AWS Cloudformation stacks for self-managed nodes.
    * SelfManagedNodeStacksRole: (Optional) The ARN of an IAM role that AWS CloudFormation assumes to delete the Self-managed Node Stacks. AWS CloudFormation uses the role's credentials to make calls on your behalf
    * AutomationAssumeRole: (Optional) The ARN of the role that allows Automation to perform the actions on your behalf.

    ## Output parameters
    * DeleteNodeGroups.output
        * DeletedNodeGroups - Deleted EKS node groups.
        * RemainingNodeGroups

    * DeleteFargateProfiles.output
        * DeletedFargateProfiles - Deleted EKS fargate profiles.
        * RemainingFargateProfiles

    * DeleteSelfManagedNodes.output
        * Stacks: Deleted self-managed node stacks in the cluster
            * Name: stack name
            * StackStatus: 'DELETE_COMPLETE'

    * DeleteEKSCluster.output
        * EKSClusterStatus
            * EKSClusterName: Deleted EKS Cluster name.
            * DeleteStatus: 'DELETING'

    * DeleteVPCCloudFormationStack.output - The Status of the deleted VPC AWS CloudFormation stack
        * Name: stack name
        * StackStatus: 'DELETE_COMPLETE'

schemaVersion: '0.3'
assumeRole: '{{AutomationAssumeRole}}'
parameters:
  EKSClusterName:
    type: String
    description: (Required) The name of the Amazon EKS Cluster to be deleted.
    allowedPattern: "^[A-Za-z0-9_-]*$"
  VPCCloudFormationStack:
    type: String
    description: (Optional) AWS Cloudformation stack name for VPC for the EKS cluster being deleted. This will delete the AWS Cloudformation stack for VPC.
    default: ''
    allowedPattern: "^[a-zA-Z0-9-]*$"
  VPCCloudFormationStackRole:
    type: String
    description: (Optional) The ARN of an IAM role that AWS CloudFormation assumes to delete the VPC CloudFormation stack. AWS CloudFormation uses the role's credentials to make calls on your behalf.
    default: ''
    allowedPattern: "^arn:aws(-cn|-us-gov)?:iam::\\d{12}:role/[\\w+=,.@-]+|^$"
  SelfManagedNodeStacks:
    type: StringList
    description: (Optional) Comma separated list of AWS Cloudformation stack names for self-managed nodes, This will delete the AWS Cloudformation stacks for self-managed stacks.
    minItems: 0
    displayType: textarea
    default: [""]
    allowedPattern: "^[a-zA-Z0-9-,]*$"
  SelfManagedNodeStacksRole:
    type: String
    description: (Optional) The ARN of an IAM role that AWS CloudFormation assumes to delete the Self-managed Node Stacks. AWS CloudFormation uses the role's credentials to make calls on your behalf.
    default: ''
    allowedPattern: "^arn:aws(-cn|-us-gov)?:iam::\\d{12}:role/[\\w+=,.@-]+|^$"
  AutomationAssumeRole:
    type: String
    description: (Optional) The ARN of the role that allows Automation to perform the actions on your behalf.
    default: ''
    allowedPattern: "^arn:aws(-cn|-us-gov)?:iam::\\d{12}:role/[\\w+=,.@-]+|^$"

outputs:
  - DeleteEKSCluster.output
  - DeleteNodeGroups.output
  - DeleteFargateProfiles.output
  - DeleteSelfManagedNodes.output
  - DeleteVPCCloudFormationStack.output
mainSteps:
- name: DeleteNodeGroups
  action: aws:executeScript
  onFailure: Abort
  isCritical: true
  timeoutSeconds: 600
  description: |
    ## DeleteNodeGroups
    Find and delete all node groups in the EKS cluster.
    ## Outputs
    * DeletedNodeGroups
    * RemainingNodeGroups
  inputs:
    Runtime: python3.7
    Handler: delete_node_groups_handler
    InputPayload:
        EKSClusterName: '{{EKSClusterName}}'
    Script: |
        import json
        import boto3
        import time

        eks = boto3.client('eks')
        DEFAULT_SLEEP_TIME=10

        def delete_nodegroups(eks_cluster_name):
            nodegroups = []
            remaining_nodegroups = []
            temp_remaining_nodegroups = []
            nodegroups_response = eks.list_nodegroups(
                clusterName=eks_cluster_name,
                maxResults=100
            )
            nodegroups = nodegroups_response['nodegroups']
            if "nextToken" in nodegroups_response:
                while "nextToken" in nodegroups_response:
                    nodegroups_response = eks.list_nodegroups(
                    clusterName=eks_cluster_name,
                    maxResults=100,
                    nextToken=nodegroups_response['nextToken']
                    )
                    nodegroups += nodegroups_response['nodegroups']

            for count,node in enumerate(nodegroups):
                delete_nodegroup_response = eks.delete_nodegroup(
                clusterName=eks_cluster_name,
                nodegroupName=node
                )
                if delete_nodegroup_response['nodegroup']['status'] != "DELETING":
                    remaining_nodegroups.append(node)

            for x in range(2):
                if not remaining_nodegroups:
                    break
                time.sleep(DEFAULT_SLEEP_TIME)
                for count,node in enumerate(remaining_nodegroups):
                    delete_nodegroup_response = eks.delete_nodegroup(
                    clusterName=eks_cluster_name,
                    nodegroupName=node
                    )
                    if delete_nodegroup_response['nodegroup']['status'] == "DELETING":
                        temp_remaining_nodegroups.append(node)
                remaining_nodegroups = temp_remaining_nodegroups
                temp_remaining_nodegroups  =[]
            return nodegroups,remaining_nodegroups

        def delete_node_groups_handler(event, context):
                eks_cluster_name = event['EKSClusterName']
                successful = True
                deleted_nodegroups = []
                remaining_nodegroups =[]
                msg= ''
                try:
                    deleted_nodegroups,remaining_nodegroups = delete_nodegroups(eks_cluster_name)
                except Exception as e:
                    successful= False
                    msg= str(e)

                out ={
                    "DeletedNodeGroups": deleted_nodegroups,
                    "RemainingNodeGroups": remaining_nodegroups
                    }

                if not successful:
                    raise Exception(msg,out)

                return {
                'output': json.dumps(out)
                }
  outputs:
    - Name: output
      Selector: $.Payload.output
      Type: String

- name: DeleteFargateProfiles
  action: aws:executeScript
  onFailure: Continue
  isCritical: false
  timeoutSeconds: 600
  description: |
    ## DeleteFargateProfiles
    Find and delete all Fargate profiles in the EKS cluster.
    ## Outputs
    * DeletedFargeteProfiles
    * RemainingFargateProfiles
  inputs:
    Runtime: python3.7
    Handler: delete_fargate_profiles_handler
    InputPayload:
        EKSClusterName: '{{EKSClusterName}}'
    Script: |
        import json
        import boto3
        import time
        eks = boto3.client('eks')

        delete_fargate_profile_retrials = 0
        remaining_fargate_profiles= []
        deleted_fargate_profiles= []

        MAX_RETRIALS_NUM= 10
        DEFAULT_SLEEP_TIME=20


        def delete_fargate_profile(eks_cluster_name, profile):
            global delete_fargate_profile_retrials
            global remaining_fargate_profiles
            global deleted_fargate_profiles
            try:
                response = eks.delete_fargate_profile(
                clusterName = eks_cluster_name,
                fargateProfileName=profile
                )

                if response['fargateProfile']['status'] != "DELETING":
                    if profile not in remaining_fargate_profiles:
                        remaining_fargate_profiles.append(profile)
                else:
                    deleted_fargate_profiles.append(profile)

            except Exception as e:
                time.sleep(DEFAULT_SLEEP_TIME)
                if delete_fargate_profile_retrials < MAX_RETRIALS_NUM:
                    delete_fargate_profile_retrials+=1
                    delete_fargate_profile(eks_cluster_name,profile)
                else:
                    raise e
            delete_fargate_profile_retrials = 0

        def delete_fargate_profiles(eks_cluster_name):
            fargate_profiles = []
            fargate_profiles_response = eks.list_fargate_profiles(
                maxResults=100,
                clusterName = eks_cluster_name
            )
            fargate_profiles = fargate_profiles_response['fargateProfileNames']
            if fargate_profiles:
                while "nextToken" in fargate_profiles_response:
                    fargate_profiles_response = eks.list_fargate_profiles(
                        maxResults=100,
                        clusterName=eks_cluster_name,
                        nextToken=fargate_profiles_response["nextToken"]
                    )
                    fargate_profiles += fargate_profiles_response['fargateProfileNames']

                for count,profile in enumerate(fargate_profiles):
                    delete_fargate_profile(eks_cluster_name,profile)

                # retry to delete the remaining fargate profiles
                for x in range(2):
                    if not remaining_fargate_profiles:
                        break
                    time.sleep(DEFAULT_SLEEP_TIME)

                    for count,profile in enumerate(remaining_fargate_profiles):
                        delete_fargate_profile(eks_cluster_name,profile)

                    # remove successfully deleted fargate profile after retries
                    for count,profile in enumerate(remaining_fargate_profiles):
                        if profile in deleted_fargate_profiles:
                            remaining_fargate_profiles.remove(profile)

        def delete_fargate_profiles_handler(event, context):
                eks_cluster_name = event['EKSClusterName']
                successful = True
                global deleted_fargate_profiles
                msg=''

                try:
                    delete_fargate_profiles(eks_cluster_name)
                except Exception as e:
                    successful= False
                    msg= str(e)

                out ={
                    "DeletedFargeteProfiles": deleted_fargate_profiles,
                    "RemainingFargateProfiles": remaining_fargate_profiles
                    }

                if not successful:
                    raise Exception(msg,out)
                elif remaining_fargate_profiles:
                    raise Exception("Not able to delete all fargate profiles",out)

                return {
                'output': json.dumps(out)
                }
  outputs:
    - Name: output
      Selector: $.Payload.output
      Type: String

- name: DeleteSelfManagedNodes
  action: aws:executeScript
  onFailure: Abort
  isCritical: True
  timeoutSeconds: 600
  description: |
    ## DeleteSelfManagedNodes
    Delete all self-managed node AWS CloudFormation stacks
    ## Outputs
    * SelfManagedNodeStacksStatus: Deleted self-managed node stacks in the cluster.
        * Stacks: Deleted self-managed node stacks in the cluster
            * Name: stack name,
            * StackStatus: 'DELETE_IN_PROGRESS'
  inputs:
    Runtime: python3.7
    Handler: delete_self_managed_nodes_handler
    InputPayload:
        SelfManagedNodeStacks: '{{SelfManagedNodeStacks}}'
        SelfManagedNodeStacksRole: '{{SelfManagedNodeStacksRole}}'
    Script: |
        import json
        import boto3
        import time
        import random
        sts_client = boto3.client('sts')
        cfn_resource = boto3.resource('cloudformation')
        cfn_client = boto3.client('cloudformation')

        DEFAULT_SLEEP_TIME=10
        current_stack_status = "undefined"
        retrials=0
        MAX_RETRIALS=10

        def is_stack_present(stack_name):
            try:
                resp = describe_stacks_with_retries(stack_name)
                stacks = resp.get("Stacks", [])
                    # double check for deleted stacks in case a stack id was used
                return any([s["StackStatus"] != "DELETE_COMPLETE" for s in stacks])
            except Exception as ex:
                if (isinstance(ex, boto3.exceptions.botocore.exceptions.ClientError)) and ('Error' in ex.response) and ('Message' in ex.response['Error']) and (ex.response['Error']['Message'].endswith("does not exist")) :
                    return False
                raise ex

        def describe_stacks_with_retries(stack_name):
            allowed_retries = 10
            finished_retry_count = 0
            for finished_retry_count in range(0, allowed_retries):
                try:
                    response = cfn_client.describe_stacks(StackName=stack_name)
                    return response
                except Exception as ex:
                    if (isinstance(ex, boto3.exceptions.botocore.exceptions.ClientError)) and \
                            ('Error' in ex.response) and \
                            ('Message' in ex.response['Error']) and \
                            ('Throttling' in ex.response['Error']['Message']):
                        # if multiple tests gets throttled at the same time, then adding constant value here would cause the
                        # next requests to be throttled again. So randomly choosing values between 1 to 3 to sleep.
                        time_to_sleep = random.randint(1, 3)
                        time.sleep(time_to_sleep)
                    else:
                        raise ex
            if finished_retry_count == allowed_retries:
                raise Exception("Automation is throttled by DescribeStacks API and is not successful even after retries!")


        def delete_cfn_stack(stack_name,role_arn):
            global retrials
            global current_stack_status

            stack_name = stack_name.strip()
            if not stack_name:
                current_stack_status = "empty_stack_name"
                return

            if not is_stack_present(stack_name):
                current_stack_status = "stack_not_found"
                return

            stack = cfn_resource.Stack(stack_name)

            if role_arn:
                stack.delete(RoleARN=role_arn)
            else:
                stack.delete()
                stack.load()

            if stack.stack_status in ["DELETE_COMPLETE","DELETE_IN_PROGRESS"]:
                # waiting for stack to be DELETE_COMPLETE
                while is_stack_present(stack_name) is True:
                    time.sleep(30)

                current_stack_status = "DELETE_COMPLETE"
            elif stack.stack_status == "DELETE_FAILED":
                raise Exception ("DELETE_FAILED stack:{} ".format(stack_name))
            else:
                # retry
                time.sleep(DEFAULT_SLEEP_TIME)
                if retrials < MAX_RETRIALS:
                    retrials += 1
                    delete_cfn_stack(stack_name,role_arn)

                else:
                    raise Exception ("not able to delete stack:{}".format(stack_name))
            # reset retrials after successful delete
            retrials = 0

        def verify_role_created(role_arn):
            # For what ever reason assuming a role that got created too fast fails, so we just wait until we can.
            retry_count = 12
            while True:
                try:
                    sts_client.assume_role(RoleArn=role_arn, RoleSessionName="checking_assume")
                    break
                except Exception as e:
                    retry_count -= 1
                    if retry_count == 0:
                        raise e
                    time.sleep(5)

        def delete_self_managed_nodes_handler(event, context):
                self_managed_node_stacks =  event['SelfManagedNodeStacks']
                self_managed_node_stacks_role = event['SelfManagedNodeStacksRole']
                global current_stack_status

                self_managed_node_stacks_status = []
                successful = True
                err_msg=''

                if self_managed_node_stacks_role:
                    verify_role_created(self_managed_node_stacks_role)


                for count,stack in enumerate(self_managed_node_stacks):
                    if stack:
                        try:
                            delete_cfn_stack(stack,self_managed_node_stacks_role)
                            stack={"Name":stack, "StackStatus":current_stack_status}
                        except Exception as e:
                            err_msg =str(e)
                            stack={"Name":stack, "StackStatus":"DELETE_FAILED","Exception":err_msg}
                            successful = False
                        finally:
                            self_managed_node_stacks_status.append(stack)
                            current_stack_status = "undefined"

                out ={
                "SelfManagedNodeStacksStatus": self_managed_node_stacks_status
                }

                if not successful:
                    raise Exception("Not able to delete all self-managed nodes stacks",out)

                return {
                'output': json.dumps(out)
                }
  outputs:
    - Name: output
      Selector: $.Payload.output
      Type: String

- name: DeleteEKSCluster
  action: aws:executeScript
  onFailure: Abort
  isCritical: true
  timeoutSeconds: 600
  description: |
    ##DeleteEKSCluster
    Delete EKS Cluster
    ## Outputs
    * EKSClusterStatus
        * EKSClusterName: Deleted EKS Cluster name.
        * DeleteStatus: 'DELETING'
  inputs:
    Runtime: python3.7
    Handler: delete_eks_cluster_handler
    InputPayload:
        EKSClusterName: '{{EKSClusterName}}'
    Script: |
        import json
        import boto3
        import time
        eks = boto3.client('eks')

        delete_eks_cluster_retrials = 0
        eks_delete_status = "Failed"
        MAX_RETRIALS_NUM= 10
        DEFAULT_SLEEP_TIME=30

        def delete_eks_cluster(eks_cluster_name):
            global delete_eks_cluster_retrials
            global eks_delete_status
            try:
                response = eks.delete_cluster(
                    name=eks_cluster_name
                )
                eks_delete_status = response['cluster']['status']
            except Exception as e:
                time.sleep(DEFAULT_SLEEP_TIME)
                if delete_eks_cluster_retrials < MAX_RETRIALS_NUM:
                    delete_eks_cluster_retrials += 1
                    delete_eks_cluster(eks_cluster_name)
                else:
                    raise e

        def delete_eks_cluster_handler(event, context):
                eks_cluster_name = event['EKSClusterName']
                error_msg=''

                eks_cluster_status ={
                    "Name":eks_cluster_name,
                    "DeleteStatus" : "Undefined"
                }
                successful = True

                try:
                    delete_eks_cluster(eks_cluster_name)
                    eks_cluster_status["DeleteStatus"] = eks_delete_status
                except Exception as e:
                    successful= False
                    eks_cluster_status["DeleteStatus"] = "Failed"
                    error_msg= str(e)


                out ={
                    "EKSClusterStatus": eks_cluster_status
                    }

                if not successful:
                    raise Exception(error_msg,out)

                return {
                'output': json.dumps(out)
                }
  outputs:
    - Name: output
      Selector: $.Payload.output
      Type: String

- name: DeleteVPCCloudFormationStack
  action: aws:executeScript
  onFailure: Continue
  isCritical: False
  timeoutSeconds: 600
  description: |
    ##  DeleteVPCCloudFormationStack
    Delete the VPC AWS CloudFormation stack
    ## Outputs
    * VPCCloudFormationStackStatus: Deleted VPC AWS CloudFormation stack.
        * Name: stack name
        * StackStatus: 'DELETE_IN_PROGRESS'
  inputs:
    Runtime: python3.7
    Handler: delete_vpc_cloudformation_stack_handler
    InputPayload:
        VPCCloudFormationStack: '{{VPCCloudFormationStack}}'
        VPCCloudFormationStackRole: '{{VPCCloudFormationStackRole}}'
    Script: |
        import json
        import boto3
        import time
        import random

        sts_client = boto3.client('sts')
        cfn_resource = boto3.resource('cloudformation')
        cfn_client = boto3.client('cloudformation')

        DEFAULT_SLEEP_TIME=10
        current_stack_status = "undefined"
        retrials=0
        MAX_RETRIALS=10

        def is_stack_present(stack_name):
            try:
                resp = describe_stacks_with_retries(stack_name)
                stacks = resp.get("Stacks", [])
                    # double check for deleted stacks in case a stack id was used
                return any([s["StackStatus"] != "DELETE_COMPLETE" for s in stacks])
            except Exception as ex:
                if (isinstance(ex, boto3.exceptions.botocore.exceptions.ClientError)) and ('Error' in ex.response) and ('Message' in ex.response['Error']) and (ex.response['Error']['Message'].endswith("does not exist")) :
                    return False
                raise ex

        def describe_stacks_with_retries(stack_name):
            allowed_retries = 10
            finished_retry_count = 0
            for finished_retry_count in range(0, allowed_retries):
                try:
                    response = cfn_client.describe_stacks(StackName=stack_name)
                    return response
                except Exception as ex:
                    if (isinstance(ex, boto3.exceptions.botocore.exceptions.ClientError)) and \
                            ('Error' in ex.response) and \
                            ('Message' in ex.response['Error']) and \
                            ('Throttling' in ex.response['Error']['Message']):
                        # if multiple tests gets throttled at the same time, then adding constant value here would cause the
                        # next requests to be throttled again. So randomly choosing values between 1 to 3 to sleep.
                        time_to_sleep = random.randint(1, 3)
                        time.sleep(time_to_sleep)
                    else:
                        raise ex
            if finished_retry_count == allowed_retries:
                raise Exception("Automation is throttled by DescribeStacks API and is not successful even after retries!")


        def delete_cfn_stack(stack_name,role_arn):
            global retrials
            global current_stack_status

            stack_name = stack_name.strip()
            if not stack_name:
                current_stack_status = "empty_stack_name"
                return

            if not is_stack_present(stack_name):
                current_stack_status = "stack_not_found"
                return

            stack = cfn_resource.Stack(stack_name)

            if role_arn:
                stack.delete(RoleARN=role_arn)
            else:
                stack.delete()
                stack.load()

            if stack.stack_status in ["DELETE_COMPLETE","DELETE_IN_PROGRESS"]:
                # waiting for stack to be DELETE_COMPLETE
                while is_stack_present(stack_name) is True:
                    time.sleep(30)

                current_stack_status = "DELETE_COMPLETE"
            elif stack.stack_status == "DELETE_FAILED":
                raise Exception ("DELETE_FAILED stack:{} ".format(stack_name))
            else:
                # retry
                time.sleep(DEFAULT_SLEEP_TIME)
                if retrials < MAX_RETRIALS:
                    retrials += 1
                    delete_cfn_stack(stack_name,role_arn)
                else:
                    raise Exception ("not able to delete stack:{}".format(stack_name))
            # reset retrials after successful delete
            retrials = 0

        def verify_role_created(role_arn):
            # For what ever reason assuming a role that got created too fast fails, so we just wait until we can.
            retry_count = 12
            while True:
                try:
                    sts_client.assume_role(RoleArn=role_arn, RoleSessionName="checking_assume")
                    break
                except Exception as e:
                    retry_count -= 1
                    if retry_count == 0:
                        raise e
                    time.sleep(5)

        def delete_vpc_cloudformation_stack_handler(event, context):
                vpc_cfn_stack_name = event['VPCCloudFormationStack']
                vpc_cfn_role = event['VPCCloudFormationStackRole']
                successful = True
                errMsg = ''
                VPCCloudFormationStackStatus = {
                        "Name": vpc_cfn_stack_name,
                        "StackStatus": current_stack_status
                        }
                vpc_cfn_stack_name.strip()
                if vpc_cfn_stack_name:
                    try:
                        if vpc_cfn_role:
                            verify_role_created(vpc_cfn_role)

                        delete_cfn_stack(vpc_cfn_stack_name,vpc_cfn_role)
                        VPCCloudFormationStackStatus["StackStatus"]  = current_stack_status

                    except Exception as e:
                        successful= False
                        VPCCloudFormationStackStatus["StackStatus"] = "DELETE_FAILED"
                        errMsg= str(e)

                out ={
                    "VPCCloudFormationStackStatus": VPCCloudFormationStackStatus
                    }

                if not successful:
                    raise Exception(errMsg,out)

                return {
                'output': json.dumps(out)
                }

  outputs:
    - Name: output
      Selector: $.Payload.output
      Type: String
	2020-12-16T23:19:56.927000+01:00	YAML	Automation	1	AWS-DeleteEKSCluster	Active
