description: "The **AWSSupport-TroubleshootS3PublicRead** helps diagnose Amazon Simple
   Storage Service (Amazon S3) public objects read access issues by analyzing the
   S3 bucket and a limited number of objects configuration including: the account
   and bucket 'block public access' settings, the bucket policy, objects ACL, objects
   server side encryption, etc."
schemaVersion: "0.3"
assumeRole: "{{AutomationAssumeRole}}"
parameters:
  AutomationAssumeRole:
    type: "String"
    description: "(Optional) The ARN of the role that allows Automation to perform
       the actions on your behalf. If no role is specified, Systems Manager Automation
       uses your IAM permissions to execute this document."
    default: ""
  S3BucketName:
    type: "String"
    description: "(Required) Specify the name of your Amazon S3 bucket."
    allowedPattern: "^[a-zA-Z0-9.-_]{1,255}$"
    maxChars: 255
  S3PrefixName:
    type: "String"
    description: "(Optional) Specify the prefix or name of the object key(s) residing
       in your Amazon S3 bucket. E.g: keyname, key*, level1/, or level1/keyname."
    default: ""
    allowedPattern: "^[a-zA-Z0-9.-_!*'()/]{0,1024}$"
    maxChars: 1024
  StartAfter:
    type: "String"
    description: "(Optional) StartAfter is the key name where you want the document
       to start listing from. StartAfter can be any key in the bucket."
    default: ""
    allowedPattern: "^[a-zA-Z0-9.-_!*'()/]{0,1024}$"
    maxChars: 1024
  MaxObjects:
    type: "Integer"
    description: "Maximum number of objects returned for analysis (between 1 and 25)."
    allowedPattern: "^[1-9]|0[1-9]|1[0-9]|2[0-5]$"
    default: 5
    allowedValues:
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    - 12
    - 13
    - 14
    - 15
    - 16
    - 17
    - 18
    - 19
    - 20
    - 21
    - 22
    - 23
    - 24
    - 25
  IgnoreBlockPublicAccess:
    type: "Boolean"
    description: "Specify if you want to ignore the account and bucket block public
       access settings. Changing this option is not recommended. Changing this option
       to 'true', causes the document analysis to not consider public access settings
       that might be blocking public read access to your objects."
    default: false
    allowedValues:
    - false
    - true
  HttpGet:
    type: "Boolean"
    description: "Specify if you want the automation document to perform a partial
       HTTP GET request of the object. The document only retrieves the first 100
       bytes using the Range HTTP header."
    default: true
    allowedValues:
    - true
    - false
  Verbose:
    type: "Boolean"
    description: "Specify if you want to see detailed information during the analysis
       or only warning and error messages."
    default: false
    allowedValues:
    - false
    - true
  CloudWatchLogGroupName:
    type: "String"
    description: "(Optional) CloudWatch Log Group Name you want to send the analysis
       result and log data. If you specify a name and it does not exist, the SSM
       Automation document will try to create it on your behalf."
    default: ""
    allowedPattern: "^[.-_/#A-Za-z0-9]{0,512}$"
    maxChars: 512
  CloudWatchLogStreamName:
    type: "String"
    description: "(Optional) CloudWatch Log Stream Name you want to send the analysis
       result and log data. If does not exist, the SSM Automation document will try
       to create it on your behalf. If you leave this input parameter empty, the
       document will use the SSM Automation execution Id as the name."
    default: ""
    allowedPattern: "^[.-_/#A-Za-z0-9]{0,512}$"
    maxChars: 512
  ResourcePartition:
    type: "String"
    description: "(Required) The partition in which the S3 bucket is located. The
       partition is used for the bucket policy simulation."
    default: "aws"
    allowedValues:
    - "aws"
    - "aws-us-gov"
    - "aws-cn"
mainSteps:
- name: "TestBucketAccess"
  action: "aws:assertAwsResourceProperty"
  description: "test description"
  onFailure: "Abort"
  inputs:
    Service: "s3"
    Api: "HeadBucket"
    Bucket: "{{S3BucketName}}"
    PropertySelector: "$.ResponseMetadata.HTTPStatusCode"
    DesiredValues:
    - "200"
- name: "GetBucketInformation"
  action: "aws:executeScript"
  timeoutSeconds: 180
  onFailure: "Abort"
  inputs:
    Runtime: "python3.7"
    Handler: "get_bucket_information"
    InputPayload:
      S3BucketName: "{{S3BucketName}}"
    Script: "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.

      #
# Permission is hereby granted, free of charge, to any person obtaining a
       copy of this
# software and associated documentation files (the "Software"
      ), to deal in the Software
# without restriction, including without limitation
       the rights to use, copy, modify,
# merge, publish, distribute, sublicense,
       and/or sell copies of the Software, and to
# permit persons to whom the Software
       is furnished to do so.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY
       OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES
       OF MERCHANTABILITY, FITNESS FOR A
# PARTICULAR PURPOSE AND NONINFRINGEMENT.
       IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM,
       DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE,
       ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER
       DEALINGS IN THE SOFTWARE.

import sys
import boto3
import json
import
       logging
from botocore.errorfactory import ClientError

sys.tracebacklimit=0

      client = boto3.client('s3')

def get_bucket_location(bucket_name):
    try:

              location = client.get_bucket_location(Bucket=bucket_name).get('LocationConstraint',{})

              if location is None:
            location = 'us-east-1'
        elif
       location == 'EU':
            location = 'eu-west-1'
        return location

          
    except ClientError as ex:
        print(f'Error: {ex.response}')

       
def get_account_canonical_id():
    try:
        return client.list_buckets().get('Owner',
       {}).get('ID')
    
    except ClientError as ex:
        print(f'Error:
       {ex.response}')

def get_bucket_information(events,context):
    """
      
    Description:
        Calls s3.GetBucketLocation to retrieve the bucket
       location
        Calls s3.ListBuckets (--query Owner.ID) to get the Amazon
       S3 canonical user id
    
    Permission Required:
        - 's3:GetBucketLocation'

              - 's3:ListBuckets'
    
    Arguments:
        events.bucket_name
       (str): Name of the S3 bucket
        
    Returns:
        bucket_location
       (str): S3 bucket location
        canonical_id (str): S3 bucket owner canonical
       user id
    """
    bucket_name = events['S3BucketName']

    try:

              return {
            'bucket_location':get_bucket_location(bucket_name),

                  'canonical_id':get_account_canonical_id()
        }
        
              
    except Exception as e:
        print(f'Error: {str(e)}')
 
             raise
    
"
  outputs:
  - Name: "error"
    Selector: "$.Payload.error"
    Type: "Boolean"
  - Name: "bucket_location"
    Selector: "$.Payload.bucket_location"
    Type: "String"
  - Name: "canonical_id"
    Selector: "$.Payload.canonical_id"
    Type: "String"
- name: "GetBlockPublicAccess"
  action: "aws:executeScript"
  timeoutSeconds: 180
  onFailure: "Continue"
  inputs:
    Runtime: "python3.7"
    Handler: "get_block_public_access"
    InputPayload:
      AccountId: "{{global:ACCOUNT_ID}}"
      S3BucketName: "{{S3BucketName}}"
      IgnoreBlockPublicAccess: "{{IgnoreBlockPublicAccess}}"
    Script: "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.

      #
# Permission is hereby granted, free of charge, to any person obtaining a
       copy of this
# software and associated documentation files (the "Software"
      ), to deal in the Software
# without restriction, including without limitation
       the rights to use, copy, modify,
# merge, publish, distribute, sublicense,
       and/or sell copies of the Software, and to
# permit persons to whom the Software
       is furnished to do so.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY
       OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES
       OF MERCHANTABILITY, FITNESS FOR A
# PARTICULAR PURPOSE AND NONINFRINGEMENT.
       IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM,
       DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE,
       ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER
       DEALINGS IN THE SOFTWARE.

import sys
import boto3
import json
from botocore.errorfactory
       import ClientError

sys.tracebacklimit=0
client = boto3.client('s3')


      def _get_public_access_settings(account_id,bucket_name=None):
    """

          Description:
        Calls s3control.GetPublicAccessBlock and s3.GetPublicAccessBlock
       to retrieves the 
        PublicAccessBlock configuration for the Amazon
       S3 bucket and the AWS Account.
        Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html

          
    Permission Required:
        - 's3:GetBucketPublicAccessBlock'

          
    Arguments:
        bucket_name (str): Name of the S3 bucket
  
            account_id (str): AWS Account Id
    """
        
    try:
  
            if bucket_name is None:
            settings = boto3.client('s3control').get_public_access_block(AccountId=account_id)

              else:
            settings = client.get_public_access_block(Bucket=bucket_name)

              
        return settings['PublicAccessBlockConfiguration']
    

          except ClientError as ex:
        if ex.response['Error']['Code'] == 'NoSuchPublicAccessBlockConfiguration':

                  return {
                'BlockPublicAcls': False,
         
             'IgnorePublicAcls': False,
                'BlockPublicPolicy': False,

                      'RestrictPublicBuckets': False
            }
        else:

                  print(f'Error: {ex.response}')
            raise

def get_block_public_access(events,context):

          """
    Description:
        Retrieves the PublicAccessBlock configuration
       for an Amazon S3 bucket and the AWS Account.
    
    Permission Required:

              - 's3:GetBucketPublicAccessBlock'
    
    Arguments:
        events.bucket_name
       (str): Name of the S3 bucket
        events.account_id (str): Caller AWS
       Account Id
        events.ignore_block_public (bool): IgnoreBlockPublicAccess
       automation input parameter. Ignores Public Access Settings.
    
    Returns:

              block_ignore_public_acl (bool): IgnorePublicAcls setting ON/OFF
 
             block_ignore_public_policy (bool): RestrictPublicBuckets setting ON/OFF

          """
    account_id = events['AccountId']
    bucket_name = events['S3BucketName']

          ignore_block_public = events['IgnoreBlockPublicAccess']
    
    block_ignore_public_acl
       = False
    block_ignore_public_policy = False

    try:
        if ignore_block_public:

                  print(f'Warning: Automation document IgnoreBlockPublicAccess input
       parameter is set to "{ignore_block_public}"')
            print('This option
       causes the document analysis to not consider public access settings that might
       be blocking public read access to your objects.')
        
        # Get
       Block public access settings for Account (account_id)
        account_settings
       = _get_public_access_settings(account_id)
        print(f'Block public access
       settings (Account): {account_settings}')
        
        if account_settings.get('IgnorePublicAcls'):

                  print(f'Warning: (Account) Block public access to buckets and objects
       granted through any access control lists (ACLs).')
            block_ignore_public_acl
       = True
        if account_settings.get('RestrictPublicBuckets'):
      
            print('Warning: (Account) Block public and cross-account access to buckets
       and objects through any public bucket or access point policies.')
      
            block_ignore_public_policy = True
        
        # Get Block public
       access settings for Bucket (bucket_name)
        settings = _get_public_access_settings(account_id,bucket_name)

              print(f'Block public access settings (Bucket): {settings}')
     
         
        if settings.get('IgnorePublicAcls'):
            print('Warning:
       (Bucket) Block public access to buckets and objects granted through any access
       control lists (ACLs).')
            block_ignore_public_acl = block_ignore_public_acl
       or True
        if settings.get('RestrictPublicBuckets'):
            print('Warning:
       (Bucket) Block public and cross-account access to buckets and objects through
       any public bucket or access point policies.')
            block_ignore_public_policy
       = block_ignore_public_policy or True
        
        # If S3 block public
       access prevents public access and the document is not ignoring them, return
       and error
        if block_ignore_public_acl and block_ignore_public_policy
       and not ignore_block_public:
            raise Exception('Public access settings
       are configured to ignore both public ACLs and any public bucket policy.')

                          
        return { 'block_ignore_public_acl':block_ignore_public_acl,

                       'block_ignore_public_policy':block_ignore_public_policy }

          
    except Exception as e:
        print(f'Error: {str(e)}')
     
         raise
    
"
  outputs:
  - Name: "block_ignore_public_acl"
    Selector: "$.Payload.block_ignore_public_acl"
    Type: "Boolean"
  - Name: "block_ignore_public_policy"
    Selector: "$.Payload.block_ignore_public_policy"
    Type: "Boolean"
- name: "CheckBucketPayer"
  action: "aws:assertAwsResourceProperty"
  onFailure: "Abort"
  isCritical: false
  inputs:
    Service: "s3"
    Api: "GetBucketRequestPayment"
    Bucket: "{{S3BucketName}}"
    PropertySelector: "$.Payer"
    DesiredValues:
    - "BucketOwner"
- name: "GetBucketPolicyStatus"
  action: "aws:executeScript"
  timeoutSeconds: 180
  onFailure: "Abort"
  inputs:
    Runtime: "python3.7"
    Handler: "get_bucket_policy_status"
    InputPayload:
      S3BucketName: "{{S3BucketName}}"
      PublicAccessIgnoreACL: "{{GetBlockPublicAccess.block_ignore_public_acl}}"
    Script: "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.

      #
# Permission is hereby granted, free of charge, to any person obtaining a
       copy of this
# software and associated documentation files (the "Software"
      ), to deal in the Software
# without restriction, including without limitation
       the rights to use, copy, modify,
# merge, publish, distribute, sublicense,
       and/or sell copies of the Software, and to
# permit persons to whom the Software
       is furnished to do so.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY
       OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES
       OF MERCHANTABILITY, FITNESS FOR A
# PARTICULAR PURPOSE AND NONINFRINGEMENT.
       IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM,
       DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE,
       ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER
       DEALINGS IN THE SOFTWARE.

import sys
import boto3
import json
from botocore.errorfactory
       import ClientError

sys.tracebacklimit=0
client = boto3.client('s3')


      def get_bucket_policy_status(events,context):
    """
    Description:

              Calls s3.GetBucketPolicyStatus to retrieve the S3 policy status indicating
       whether 
        the bucket is public (True) or not (False). Returns None
       if the bucket does 
        not have a policy associated
    
    Permission
       Required:
        - 's3:GetBucketPolicyStatus'
    
    Arguments:
  
            events.bucket_name (str): Name of the S3 bucket
        events.block_ignore_public_acl
       (bool): Block public access to buckets and objects granted through 
    
          any access control lists (ACLs) setting
    
    Returns:
        status
       (bool): is public (True) or not (False). Returns None if the bucket does 

              not have a policy associated
    """
    
    bucket_name = events['S3BucketName']

          block_ignore_public_acl = events['PublicAccessIgnoreACL']

    try:

              is_public = client.get_bucket_policy_status(Bucket=bucket_name)['PolicyStatus']['IsPublic']

              
        if is_public:
            print('Info: Bucket policy is
       marked as public.')
            return { 'status':True }
        else:

                  if block_ignore_public_acl:
                raise Exception('Bucket
       policy is marked as private and the public access settings are configured
       to ignore public ACLs.')
            else:
                print('Warning:
       Bucket policy is marked as private.')
                return { 'status':False
       }
    except ClientError as ex:
        if ex.response["Error"]["Code"
      ] == "NoSuchBucketPolicy":
            print('Info: Bucket has no policy
       associated.')
            return { 'status':None }
        else:
     
             print(f'Error: {ex.response}')
            raise

    except Exception
       as e:
        print(f'Error: {str(e)}')
        raise
    
"
  outputs:
  - Name: "status"
    Selector: "$.Payload.status"
    Type: "Boolean"
- name: "GetBucketPolicy"
  action: "aws:executeAwsApi"
  onFailure: "step:GetBucketAcl"
  isCritical: false
  inputs:
    Service: "s3"
    Api: "GetBucketPolicy"
    Bucket: "{{S3BucketName}}"
  outputs:
  - Name: "Policy"
    Selector: "$.Policy"
    Type: "String"
- name: "GetContextKeys"
  action: "aws:executeAwsApi"
  onFailure: "Continue"
  isCritical: false
  inputs:
    Service: "iam"
    Api: "GetContextKeysForCustomPolicy"
    PolicyInputList:
    - "{{GetBucketPolicy.Policy}}"
  outputs:
  - Name: "ContextKeyNames"
    Selector: "$.ContextKeyNames"
    Type: "StringList"
- name: "SimulateBucketPolicy"
  action: "aws:assertAwsResourceProperty"
  timeoutSeconds: 180
  onFailure: "Continue"
  isCritical: false
  inputs:
    Service: "iam"
    Api: "SimulateCustomPolicy"
    PolicyInputList:
    - "{{GetBucketPolicy.Policy}}"
    ResourceArns:
    - "arn:{{ResourcePartition}}:s3:::{{S3BucketName}}/{{S3PrefixName}}*"
    ActionNames:
    - "s3:GetObject"
    PropertySelector: "$.EvaluationResults[0].EvalDecision"
    DesiredValues:
    - "allowed"
    - "implicitDeny"
- name: "GetBucketAcl"
  action: "aws:executeAwsApi"
  timeoutSeconds: 180
  onFailure: "Continue"
  inputs:
    Service: "s3"
    Api: "GetBucketAcl"
    Bucket: "{{S3BucketName}}"
  outputs:
  - Name: "Owner"
    Selector: "$.Owner.ID"
    Type: "String"
  - Name: "Grants"
    Selector: "$.Grants"
    Type: "MapList"
- name: "CreateLogandStream"
  action: "aws:executeScript"
  timeoutSeconds: 180
  onFailure: "Continue"
  inputs:
    Runtime: "python3.7"
    Handler: "create_cloudwatch_log_stream"
    InputPayload:
      CloudWatchLogGroupName: "{{CloudWatchLogGroupName}}"
      CloudWatchLogStreamName: "{{CloudWatchLogStreamName}}"
      ExecutionId: "{{automation:EXECUTION_ID}}"
    Script: "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.

      #
# Permission is hereby granted, free of charge, to any person obtaining a
       copy of this
# software and associated documentation files (the "Software"
      ), to deal in the Software
# without restriction, including without limitation
       the rights to use, copy, modify,
# merge, publish, distribute, sublicense,
       and/or sell copies of the Software, and to
# permit persons to whom the Software
       is furnished to do so.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY
       OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES
       OF MERCHANTABILITY, FITNESS FOR A
# PARTICULAR PURPOSE AND NONINFRINGEMENT.
       IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM,
       DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE,
       ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER
       DEALINGS IN THE SOFTWARE.

import sys
import boto3

from botocore.errorfactory
       import ClientError

sys.tracebacklimit=0
logs = boto3.client('logs')


      def _create_log_group_if_not_exists(log_group_name,retention_days):
    ""
      "
    Description:
    Creates the Log Group if a log_group_name is provided.
       If a new Log Group is created,
    the function tries to set the Logs retention
       days to retention_days.
    
    Permission Required:
    - 'logs:CreateLogGroup'

          - 'logs:PutRetentionPolicy'
    
    Arguments:
    log_group_name:
       Name of the Log Group
    retention_days: The number of days to retain the
       log events in the specified log group.
    """
    print(f'Info: Creating
       Log Group {log_group_name}.')
    try:
        if log_group_name:
    
              logs.create_log_group(logGroupName=log_group_name)
            

                  try:
                logs.put_retention_policy(logGroupName=log_group_name,retentionInDays=retention_days)

                  except ClientError as ex:
                print(f'Error: {ex.response}')

                  
            return True
        else:
            return False

          except ClientError as ex:
        # If the Log Group exist, don't return
       an error
        if ex.response['Error']['Code'] == 'ResourceAlreadyExistsException':

                  print(f'Warning: Log Group {log_group_name} already exists.')

                  return True
        else:
            print(f'Error: {ex.response}')

                  return False

def _create_log_stream_if_not_exists(log_stream_name,log_group_name):

          """
    Description:
        Creates the Log Stream if a log_group_name
       and log_stream_name are provided.
    
    Permission Required:
      
        - 'logs:CreateLogStream'
    
    Arguments:
        log_group_name: Name
       of the Log Group
        log_stream_name: Name of the Log Stream
    ""
      "
    print(f'Info: Creating Log Stream {log_stream_name}.')
    try:
 
             if log_stream_name and log_group_name:
            logs.create_log_stream(logGroupName=log_group_name,
       logStreamName=log_stream_name)
            return True
        else:
 
                 return False
    except ClientError as ex:
        # If the Log
       Stream exist, don't return an error
        if ex.response['Error']['Code']
       == 'ResourceAlreadyExistsException':
            print(f'Warning: Log Stream
       {log_stream_name} already exists.')
            return True
        else:

                  print(f'Error: {ex.response}')
            return False
    
          
def create_cloudwatch_log_stream(events,context):
    """
    Description:

              Calls logs.CreateLogGroup and logs.CreateLogStream to create the CloudWath
       Log Group and Stream
        if requested in the document. If CloudWatchLogStreamName
       is not provided, the automation
        execution id is used. The log retention
       days is set to RETENTION_DAYS days.
    
    Permission Required:
    
          - 'logs:CreateLogGroup'
        - 'logs:CreateLogStream'
        - 'logs:PutRetentionPolicy'

          
    Arguments:
        events.CloudWatchLogGroupName: Name of the CloudWatch
       Log Group
        events.CloudWatchLogStreamName: Name of the CloudWatch
       Log Stream
        events.ExecutionId: Automation execution id (automation:EXECUTION_ID)

          """
    
    log_group_name = events.get('CloudWatchLogGroupName','')

          log_stream_name = events.get('CloudWatchLogStreamName','')
    execution_id
       = events.get('ExecutionId','')
    RETENTION_DAYS = 14
    
    # If the
       CloudWatchLogStreamName is empty, use the Automation Execution Id
    if
       not log_stream_name:
        log_stream_name = execution_id

    if not
       log_group_name:
        return { 'error':True }
          
    if log_group_name
       and log_stream_name and _create_log_group_if_not_exists(log_group_name,RETENTION_DAYS)
       and _create_log_stream_if_not_exists(log_stream_name,log_group_name):
  
            return { 'error':False }

    return { 'error':True }
"
  outputs:
  - Name: "error"
    Selector: "$.Payload.error"
    Type: "Boolean"
- name: "AnalyzeObjects"
  action: "aws:executeScript"
  timeoutSeconds: 600
  onFailure: "Abort"
  inputs:
    Runtime: "python3.7"
    Handler: "analyze_objects"
    InputPayload:
      AccountId: "{{global:ACCOUNT_ID}}"
      S3BucketName: "{{S3BucketName}}"
      S3PrefixName: "{{S3PrefixName}}"
      StartAfter: "{{StartAfter}}"
      GetBucketInformationLocation: "{{GetBucketInformation.bucket_location}}"
      GetBucketInformationOwner: "{{GetBucketInformation.canonical_id}}"
      MaxObjects: "{{MaxObjects}}"
      IgnoreBlockPublicAccess: "{{IgnoreBlockPublicAccess}}"
      CloudWatchLogGroupName: "{{CloudWatchLogGroupName}}"
      CloudWatchLogStreamName: "{{CloudWatchLogStreamName}}"
      CreateLogandStreamError: "{{CreateLogandStream.error}}"
      Verbose: "{{Verbose}}"
      HttpGet: "{{HttpGet}}"
      ExecutionId: "{{automation:EXECUTION_ID}}"
    Script: "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.

      #
# Permission is hereby granted, free of charge, to any person obtaining a
       copy of this
# software and associated documentation files (the "Software"
      ), to deal in the Software
# without restriction, including without limitation
       the rights to use, copy, modify,
# merge, publish, distribute, sublicense,
       and/or sell copies of the Software, and to
# permit persons to whom the Software
       is furnished to do so.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY
       OF ANY KIND, EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES
       OF MERCHANTABILITY, FITNESS FOR A
# PARTICULAR PURPOSE AND NONINFRINGEMENT.
       IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
# HOLDERS BE LIABLE FOR ANY CLAIM,
       DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE,
       ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER
       DEALINGS IN THE SOFTWARE.

import sys, boto3, json, time, warnings, http.client,
       textwrap
from collections import defaultdict
from botocore.errorfactory
       import ClientError

sys.tracebacklimit=0
client = boto3.client('s3')

      logs = boto3.client('logs')

# Object tags information when the bucket policy
       contains any s3:ExistingObjectTag condition key
def _get_object_tags(bucket,key_name):

          try:
        return client.get_object_tagging(Bucket=bucket,Key=key_name).get('TagSet',[])

          except ClientError as ex:
        _print_error(ex,key_name,'get object
       tags')
        return []

def _simulate_custom_policy(bucket,policy,arn,verbose=False):

          # https://github.com/boto/boto3/issues/2084
    # Policy search based
       on tags is not supported
    """
    Description:
        Simulates
       the bucket policy for 's3:GetObject' (read) action
    
    Permission Required:

              - 's3:SimulateCustomPolicy'
    
    Arguments:
        bucket (str):
       Name of the S3 bucket
        policy (str): Policy associated with the S3
       bucket
        arn: Arn of the object. Ex: 'arn:aws:s3:::{bucket}/{key_name}'

          
    Returns:
        decision (str): EvaluationResults.EvalDecision
       (allowed | explicitDeny | implicitDeny | error)
        missingContextValues
       (str): EvaluationResults.MissingContextValues A list of context keys that
       are 
            required by the included input policies but that were not
       provided by one of the input parameters
    """
    
    try:
    
          response = boto3.client('iam').simulate_custom_policy(
            PolicyInputList=[policy],

                  ResourceArns=[arn],
            ActionNames=['s3:GetObject'])

      
        results = response.get('EvaluationResults',[])
        decision =
       response['EvaluationResults'][0].get('EvalDecision','')
        missingContextValues
       = response['EvaluationResults'][0].get('MissingContextValues',[])
      
        
        if len(results) == 1:
            return decision,missingContextValues

              elif len(results) != 1:
            _print_error('simulate_custom_policy
       returned multiple values, expected only 1',arn,'simulate policy')
      
            return 'error',['error']
    except ClientError as ex:
        _print_error(ex,arn,'simulate
       policy')
        return 'error',['error']
    else:
        return 'error',['error']

      
def _scan_public_urls(bucket,bucket_location,key,request_payer=False):
 
         """
    Description:
        Performs an HTTPS Get using the Range
       bytes=0-0 header to receive only the first byte.
        When using "Range"
       it is expected to receive HTTP status 206 (Partial Content)
    
    Permission
       Required:
        - N/A
    
    Arguments:
        bucket (string): Name
       of the S3 bucket
        bucket_location (string): ocation of the bucket
       (region)
        key (string): object key name
        request_payer (bool):
       if True, sets the x-amz-request-payer:requester header
    
    Returns:

              response.status. Ex: 200, 206, 400, etc.
        response.reason.
       Ex: 206 Partial Content, 200 OK, 400 Bad Request, etc.
        req_id: 'x-amz-id-2'
       request header
        host_id: 'x-amz-request-id' request header
    "
      ""
    
    try:
        url=f'{bucket}.s3.{bucket_location}.amazonaws.com'

              connection = http.client.HTTPSConnection(url,timeout=5)
        if
       request_payer:
            connection.request('GET', f'/{key}',headers={"
      Range":"bytes=0-0","x-amz-request-payer":"requester"})
        else:
          
            connection.request('GET', f'/{key}',headers={"Range": "
      bytes=0-0"})
        response = connection.getresponse()
        req_id =
       response.getheader('x-amz-id-2', default=None)
        host_id = response.getheader('x-amz-request-id',
       default=None)
        connection.close()
        return response.status,
       response.reason, req_id, host_id
    except Exception as e:
        _print_error(e,key,'http
       get')
        return 0,0,'error','error'

def _get_bucket_policy(bucket):

          try:
        return client.get_bucket_policy(Bucket=bucket).get('Policy')

          except ClientError as ex:
        if ex.response['Error']['Code'] == 'NoSuchBucketPolicy':

                  return {}
        else:
            _print_error(ex,bucket,'get
       bucket policy')
            raise
    
def _check_public_acl(grants,bucket,key_name=None):

          PUBLIC_GROUP = { 'http://acs.amazonaws.com/groups/global/AllUsers': 'All
       Users' }
    # Anonymous canonical user ID https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html

          ANONYMOUS_USER = '65a011a29cdf8ec533ec3d1ccaae921c'
    
    permissions
       = set()

    for grant in grants:
        uri = grant['Grantee'].get('URI')

              uid = grant['Grantee'].get('ID')
        if uri in PUBLIC_GROUP.keys()
       and grant['Permission'] in ('READ','FULL_CONTROL'):
            permissions.add(PUBLIC_GROUP[uri])

              if uid == ANONYMOUS_USER and grant['Permission'] in ('READ','FULL_CONTROL'):

                  permissions.add(ANONYMOUS_USER)
    
    return permissions

      
def _get_bucket_payer(bucket):
    try:
        response = client.get_bucket_request_payment(Bucket=bucket)

              return response.get('Payer')
    except ClientError as ex:
     
         _print_error(ex,bucket,'get bucket payer')
        return 'error'

def
       _get_object_acl(bucket,key,caller_s3_user):
    try:
        response =
       client.get_object_acl(Bucket=bucket,Key=key)
        return response['Grants'],
       False
    except ClientError as ex:
        _print_error(ex,key,'get object
       acl')
        return [], True
    
def _check_encryption(bucket,key_name):

          try:
        response = client.head_object(Bucket=bucket,Key=key_name)

              return response.get('ServerSideEncryption')
    except ClientError
       as ex:
        _print_error(ex,key_name,'encryption')
        return 'error'

      
def _print_error(err,name,module_name='',err_type='error'):
    try:
  
            if type(err) == ClientError:
            print(f'[{err_type}] [{module_name}]
       [{name}] code:{err.response["Error"]["Code"]} message:{err.response["
      Error"]["Message"]}')
        else:
            print(f'[{err_type}] [{module_name}]
       [{name}] {str(err)}')
    except Exception as e:
        print(f'Error:
       {str(e)}')
        
def _get_object_metadata(bucket,key_name):
    try:

              response = client.head_object(Bucket=bucket,Key=key_name)
       
       return response
    except ClientError as ex:
        _print_error(ex,key_name,'head
       object')
        return {}
    
def _list_objects(bucket, prefix, start_after,
       limit):
    try:
        if limit > 100:
            raise ValueError("
      List objects MaxItems greater than 100 is not supported")
        paginator
       = client.get_paginator("list_objects_v2")
        page_iterator = paginator.paginate(Bucket=bucket,
       Prefix=prefix, StartAfter=start_after,
            FetchOwner=True, PaginationConfig={'MaxItems':
       limit})
        keys = defaultdict(str)
        for page in page_iterator:

                  if 'Contents' in page:
                for key in page['Contents']:

                          keys[key['Key']]=key['Owner']['ID']
        return keys

          except ClientError as ex:
        _print_error(ex,bucket,'list objects')

              if ex.response['Error']['Code'] == 'NoSuchBucket':
            return
       keys
        else:
            raise

# Handles the semantics of uploading
       batches to CloudWatch Logs
class CWLogger(object):
    # 1MB Max Size of
       a batch of log events (Service Limit)
    MAX_BATCH_SIZE_BYTES = 1000000

          # 10K max events in a batch (Service Limit)
    MAX_BATCH_COUNT = 9999

          # Each log event has a 26 byte overhead
    LOGEVENT_OVERHEAD_BYTES =
       26
    
    ERROR_CODES = {
        'I00':'Starting S3 bucket analysis.',

              'W01':'Automation document IgnoreBlockPublicAccess input parameter
       is set to true.',
        'E01':'S3 bucket or account block public access
       settings are configured to ignore public ACLs and any public bucket policy.',

              'I01':'Starting object analysis.',
        'W02':'Object name ending
       with a single period (.) or two periods (..), prevents you downloading the
       object using the Amazon S3 console.',
        'W03':'Object owner is canonical
       anonymous user {}.',
        'W04':'Object owner user id {}.. does not match
       the account user id {}..',
        'I02':'Object owner user id {}.. match
       the account user id {}..',
        'E02':'Block block public access settings
       are configured to ignore public ACLs and bucket policy is not used because
       the object owner is not the bucket owner.',
        'I03':'Object ACL does
       not grant public permissions.',
        'I04':'Object ACL grants read permissions
       for {}.',
        'E03':'Object ACLs is not public and bucket policy is not
       used for granting access because the object owner is not the bucket owner.
       Note: Denying access it is still used to deny access to objects the bucket
       owner does not own.',
        'E04':'Object ACL does not grant public permissions
       and the block public access settings are configured to ignore public policies.',

              'I05':'Simulate bucket policy allows s3:GetObject permissions for object.',

              'E05':'Simulate bucket policy found an s3:GetObject explicit deny that
       match object arn.',
        'W05':'Simulate bucket policy found an s3:GetObject
       implicit deny that match object arn.',
        'W09':'Simulate bucket policy
       found an s3:GetObject implicit deny for the object arn with MissingContextValues
       {}. Check the bucket policy Conditions section',
        'E06':'Neither the
       object ACL or bucket policy grant public read permissions for object.',

              'E07':'Bucket policy does not grant public permissions for object and
       block public access settings are configured to ignore public ACLs.',
   
           'I06':'Object has website redirection configured.',
        'W06':'Object
       version deleted (delete marker). For more information: https://docs.aws.amazon.com/AmazonS3/latest/dev/DeleteMarker.html.',

              'W07':'Object legal hold status is ON.',
        'E08':'Object storage
       class {} not supported for public read access.',
        'E09':'Object is
       encrypted using KMS.',
        'I07':'Object is not encrypted using KMS.',

              'I08':'No public access configuration issues detected. Please check
       the [error] or [warn] entries.',
        'W08':'Object analysis did not detect
       public access configuration issues, however the IgnoreBlockPublicAccess document
       input is enabled (True).',
        'I09':'HTTP GET request status:{}, reason:{}.',

              'E10':'HTTP GET request status:{}, reason:{}, x-amz-request-id:{},
       x-amz-id-2:{}.',
        'I10':'Object analysis finished.',
        'E11':'list_objects
       operations did not find any object with prefix:{}.',
        'E12':'S3 bucket
       analysis detected some public access configuration issues. Please review the
       AnalyzeObjects output step for more details.',
        'I11':'S3 bucket analysis
       did not detect public access configuration issues. Please review the AnalyzeObjects
       output step for more details.',
        'I12':'S3 bucket analysis finished.',

              'E13':'Unhandled error {}.',
        'W10':'Object has the following
       tags {}.',
        'E14':'Unhandled policy simulation error result EvalDecision:
       {}, MissingContextValues: {}.',
        'E15':'Error getting object ACL.',

              'E16':'Error getting object metadata.',
        'E17':'Error getting
       object server side encryption configuration.',
        'E18':'Object ACL
       does not grant public permissions and bucket has no policy associated.',

              'E19':'Object analysis detected some public access configuration issues.
       Please review the previous output for details.',
        'I13':'Object metadata
       replication status: {}.',
        'W11':'Account block public access settings
       are configured to ignore public ACLs.',
        'W12':'Account block public
       access settings are configured to ignore any public bucket policy.',
   
           'W13':'S3 bucket block public access settings are configured to ignore
       public ACLs.',
        'W14':'S3 bucket block public access settings are
       configured to ignore any public bucket policy.',

    }
    
    def __init__(self,
       log_group, log_stream, bucket, execution_id, max_retries_on_put=5, enabled=False,
       debug=False):
        # The Log Group and Log Stream
        self.log_group
       = log_group
        self.log_stream = log_stream
        # Max number of
       times to retry a failed PutLogEvents request for a batch of events
     
         self.max_retries_on_put = max_retries_on_put
        # Feature enabled

              self.enabled = enabled
        self.debug = debug
        # Current
       sequence token
        self.sequence_token = None
        # Current batch
       being buffered
        self.current_batch_size = 0
        self.current_batch
       = []

        self.output = defaultdict(list)
        self.bucket = bucket

              self.execution_id = execution_id
    
    def report(self):
   
           dic = self.output.copy()
        summary = self.prettify({self.bucket:dic.pop(self.bucket,{})})

              report = self.prettify(dic)
        return json.dumps(self.output),
       summary, report       
    
    def prettify(self,dic,indent=20,width=160):

              txt = ''
        for k,v in dic.items():
            txt += f'{k}
      n'
            for r in v:
                txt +='[{:<5}] [{:<3}] {}
'.format(r['Status'],r['Code'],r['Description'])

                  txt += '
'
        
        return txt
      
    def push_event(self,name,code,*args):

              """
        @type level: int
        @param level: 0='info' 1='warn',
       2='error', 3='debug'
        """
        level = 'debug'
        if
       code.startswith('I'):
            level = 'info'
        elif code.startswith('W'):

                  level = 'warn'
        elif code.startswith('E'):
          
        level = 'error'           
            
        timestamp = int(round(time.time()
       * 1000))

        msg = self.ERROR_CODES[code].format(*args)
        log_event
       = {
            "timestamp" : timestamp,
            "message": f'[{self.execution_id}]
       [{level}] [{name}] [{code}] {msg}'
        }
            
        event_size
       = len(log_event['message'].encode('utf-8')) + self.LOGEVENT_OVERHEAD_BYTES

              if len(self.current_batch) == self.MAX_BATCH_COUNT or event_size +
       self.current_batch_size  >= self.MAX_BATCH_SIZE_BYTES:
            self.flush()

      
        self.current_batch.append(log_event)
        self.current_batch_size
       += event_size
         
        if self.debug or (level in ('warn','error','debug'))
       or code == 'I09':
            self.output[name].append({ 'Status':level,'Code':code,'Description':self.ERROR_CODES[code].format(*args)
       })
    
    def flush(self):
        try:
            if len(self.current_batch)
       > 0 and self.enabled and self.log_group and self.log_stream:
           
           _print_error(f'Pushing batch with {len(self.current_batch)} events to
       CloudWatch','logs','flush','info')
                self.send_batch(self.current_batch)

                      self.current_batch = []
                self.current_batch_size
       = 0
        except Exception as e:
            _print_error(e,'logs','flush')

                  
    def send_batch(self, batch):
        if len(batch) > 0 and
       self.enabled:
            batch.sort(key=lambda event: event['timestamp'])

                  if self.enabled:
                try:
                    self.put_log_events_with_retry(batch)

                      except Exception as e:
                    _print_error(e,'logs','send
       batch')

    def put_log_events_with_retry(self, batch):
        retry_num
       = 0
        while retry_num < self.max_retries_on_put:
            retry_num
       += 1
            try:
                if self.debug and retry_num > 2:

                          _print_error(f'retry: {retry_num}','cloudwatch logs','put
       log event','debug')
                if self.sequence_token:
           
               response = logs.put_log_events(logGroupName=self.log_group, logStreamName=self.log_stream,
       logEvents=batch, sequenceToken=self.sequence_token)
                else:

                          response = logs.put_log_events(logGroupName=self.log_group,
       logStreamName=self.log_stream, logEvents=batch)

                self.sequence_token
       = response["nextSequenceToken"]
                return self.sequence_token

                  except Exception as e:
                if "ResourceNotFoundException"
       in str(e):
                    pass
                elif "DataAlreadyAcceptedException"
       in str(e):
                    # Skipping as this payload has already been
       accepted.
                    self.sequence_token = self.parse_next_sequence_token_from_exception(e)

                          return self.sequence_token
                elif "InvalidSequenceTokenException"
       in str(e):
                    # Re-trying with next sequence token.'
 
                         self.sequence_token = self.parse_next_sequence_token_from_exception(e)

                      else:
                    # Sleep before the retry to avoid
       throttling
                    time.sleep(0.200)
        
        _print_error(f'failed
       to put batch after {retry_num} retries','logs','flush')
        raise Exception("
      Unable to send batch.")

    def parse_next_sequence_token_from_exception(self,
       e):
        parsed_token = None
        if "sequenceToken: " in str(e):

                  # Format for DataAlreadyAcceptedExceptions
            parsed_token
       = str(e).split("sequenceToken: ")[1]
        elif "sequenceToken is: "
       in str(e):
            # Format for InvalidSequenceTokenException
     
             parsed_token = str(e).split("sequenceToken is: ")[1]
        else:

                  _print_error('could not parse the sequence token from the exception:'+str(e),'logs','parse
       next token')
            parsed_token = None

        if parsed_token ==
       "null":
            # This happens when sending log events with a token
       to
            # a stream that doesn't expect a token.
            parsed_token
       = None

        return parsed_token

def _get_public_access_settings(account_id,bucket_name=None):

          """
    Description:
        Calls s3control.GetPublicAccessBlock
       and s3.GetPublicAccessBlock to retrieves the 
        PublicAccessBlock configuration
       for the Amazon S3 bucket and the AWS Account.
        Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html

          
    Permission Required:
        - 's3:GetBucketPublicAccessBlock'

          
    Arguments:
        bucket_name (str): Name of the S3 bucket
  
            account_id (str): AWS Account Id
    """
        
    try:
  
            if bucket_name is None:
            settings = boto3.client('s3control').get_public_access_block(AccountId=account_id)

              else:
            settings = client.get_public_access_block(Bucket=bucket_name)

              
        return settings['PublicAccessBlockConfiguration']
    

          except ClientError as ex:
        if ex.response['Error']['Code'] == 'NoSuchPublicAccessBlockConfiguration':

                  return {
                'BlockPublicAcls': False,
         
             'IgnorePublicAcls': False,
                'BlockPublicPolicy': False,

                      'RestrictPublicBuckets': False
            }
        else:

                  print(f'Error: {ex.response}')
            raise
        
def
       analyze_objects(events,context):
    """
    Description:
        List
       the objects from the bucket using s3.list_objects_v2 given the Automation
       
        document parameters: S3BucketName, S3PrefixName, StartAfter, and
       MaxObjects in order
        to check if the bucket or object configuration
       that prevents the object to be public readable

    Permission Required:

              - 'logs:PutLogEvents'
        - 'iam:SimulateCustomPolicy'
     
         - 'iam:GetContextKeysForCustomPolicy'
        - 's3:GetObject'
      
        - 's3:GetObjectAcl'
        - 's3:ListBucket'
        - 's3:GetObject'

              - 's3:GetObjectAcl'
        - 's3:GetObjectTagging'
        
  
        Arguments:
        events.S3BucketName (string): Name of the S3 bucket

              events.S3PrefixName (string): Shared name prefix for objects or object
       name
        events.StartAfter (string): Starts listing after this specified
       key
        events.MaxObjects (integer): Max number of objects to be returned

              events.GetBucketInformationLocation (string): S3 bucket location

              events.GetBucketInformationOwner (string): S3 owner canonical user

              events.GetBlockPublicAccessIgnoreACL (bool): IgnorePublicAcls setting
       ON/OFF
        events.GetBlockPublicAccessIgnorePolicy (bool): RestrictPublicBuckets
       setting ON/OFF
        events.IgnoreBlockPublicAccess (bool): IgnoreBlockPublicAccess
       automation input parameter. Ignores Public Access Settings
        events.Verbose
       (bool): Return INFO and DEBUG information during the analysis
        events.CloudWatchLogGroupName
       (string): S3 bucket location
        events.CloudWatchLogStreamName (string):
       S3 owner canonical user
        events.CreateLogandStreamError (bool): True
       if CW LogGroup and Stream was created succesfully, False otherwise
     
         events.HttpGet (bool): True if you want to perform HTTP.GET
        events.ExecutionId
       (string): Automation Execution Id used for pushing logs to CW
        

          Returns:
        error (bool): True if any issue is found in the configuration,
       False otherwise
        json: analysis output in JSON format (only for step
       output)
        summary: bucket analysis section for Automation overall output

              report: object analysis section for Automation overall output
   
       """
    account_id = events['AccountId']
    bucket = events['S3BucketName']

          prefix = events.get('S3PrefixName','')
    start_after = events.get('StartAfter','')

          limit = int(events.get('MaxObjects',5))
    bucket_location = events['GetBucketInformationLocation']

          bucket_owner = events['GetBucketInformationOwner']
    ignore_block_public
       = events['IgnoreBlockPublicAccess']
    verbose = events.get('Verbose',False)

          log_group = events.get('CloudWatchLogGroupName')
    log_stream = events.get('CloudWatchLogStreamName')

          enabled = not events.get('CreateLogandStreamError',False)
    http_get
       = events.get('HttpGet',False)
    execution_id = events.get('ExecutionId','id')

          
    logger = CWLogger(log_group, log_stream, bucket, execution_id, 5,
       enabled, verbose)
    logger.push_event(bucket,'I00')
    
    block_ignore_public_acl
       = False
    block_ignore_public_policy = False
                  
    if
       ignore_block_public:
        logger.push_event(bucket,'W01')
        # The
       analysis ignores bucket and account public access settings
    else:
  
            account_settings = _get_public_access_settings(account_id)
        

              if account_settings.get('IgnorePublicAcls'):
            logger.push_event(account_id,'W11')

                  block_ignore_public_acl = True
        if account_settings.get('RestrictPublicBuckets'):

                  logger.push_event(account_id,'W12')
            block_ignore_public_policy
       = True

        settings = _get_public_access_settings(account_id,bucket)

              
        if settings.get('IgnorePublicAcls'):
            logger.push_event(bucket,'W13')

                  block_ignore_public_acl = True
        if settings.get('RestrictPublicBuckets'):

                  logger.push_event(bucket,'W14')
            block_ignore_public_policy
       = True

    if block_ignore_public_acl and block_ignore_public_policy:

              logger.push_event(bucket,'E01')
        dump, summary, report = logger.report()

              return { 'error':True, 
            'json': dump,
            'summary':summary,

                  'report':report
            }  
    
    bucket_policy = _get_bucket_policy(bucket)

          obj_num = 0
    issue_found = False
    try:
        for key_name, object_owner
       in _list_objects(bucket, prefix, start_after, limit).items():
          
        if key_name.endswith('/'):
                continue  # cut early, next object

                  obj_num += 1
            
            if block_ignore_public_acl
       and block_ignore_public_policy:
                logger.push_event(key_name,'E01')

                      issue_found = True
                continue  # cut early,
       next object
            
            if http_get:
                get_status,get_reason,req_id,host_id
       = _scan_public_urls(bucket,bucket_location,key_name)
                if get_status
       in (200,206):
                    logger.push_event(key_name,'I09',get_status,get_reason)

                          continue  # cut early, next object
                else:

                          logger.push_event(key_name,'E10',get_status,get_reason,req_id,host_id)

                          issue_found = True
                
            # https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html

                  if key_name.endswith('.') or key_name.endswith('..'):
       
               logger.push_event(key_name,'W02')
            # If OBJECT owner does
       not match caller S3 canonical user or owner is anonymus (ANONYMOUS_USER) print
       a Warning
            ANONYMOUS_USER = '65a011a29cdf8ec533ec3d1ccaae921c'

                  if object_owner != bucket_owner:
                if object_owner
       == ANONYMOUS_USER:
                    logger.push_event(key_name,'W03',ANONYMOUS_USER)

                      else:
                    logger.push_event(key_name,'W04',object_owner[:10],bucket_owner[:10])

                  else:
                # Bucket owner and object owner match

                      logger.push_event(key_name,'I02',object_owner[:10],bucket_owner[:10])

                  
            if (object_owner != bucket_owner) and block_ignore_public_acl:

                      logger.push_event(key_name,'E02')
                issue_found
       = True
                continue  # cut early, next object
            

                  object_acl, error_acl = _get_object_acl(bucket,key_name,bucket_owner)

                  object_public_acl = _check_public_acl(object_acl,bucket,key_name)

                  if error_acl:
                logger.push_event(key_name,'E15')

                      issue_found = True
            else:
                if len(object_public_acl)
       == 0:
                    logger.push_event(key_name,'I03')
           
           else:
                    for group in object_public_acl:
         
                     logger.push_event(key_name,'I04',group)
            
    
                  if len(object_public_acl) == 0 and (object_owner != bucket_owner):

                          logger.push_event(key_name,'E03')
                   
       continue  # cut early, next object
                
                if len(object_public_acl)
       == 0 and block_ignore_public_policy:
                    logger.push_event(key_name,'E04')

                          issue_found = True
                    continue  # cut
       early, next object
           
            # TODO: s3:ExistingObjectTag/<key>t
      Requires that an existing object tag has a specific tag key and value.
   
               tags = _get_object_tags(bucket,key_name)
            # Log object
       tags for s3:ExistingObjectTag condition analysis
            if len(tags)
       > 0:
                logger.push_event(key_name,'W10',tags)
           
       
            if bucket_policy:
                partition = 'aws'
     
                 if bucket_location.startswith('cn-'):
                    partition
       = 'aws-cn'
                if bucket_location.startswith('us-gov'):
   
                       partition = 'aws-us-gov'
                
             
         arn = f'arn:{partition}:s3:::{bucket}/{key_name}'
                evaluation,
       missingContextValues = _simulate_custom_policy(bucket,bucket_policy,arn,verbose)

                      if evaluation == 'allowed':
                    logger.push_event(key_name,'I05')

                      elif evaluation == 'explicitDeny':
                    logger.push_event(key_name,'E05')

                          issue_found = True
                    continue  # cut
       early, next object
                elif evaluation == 'implicitDeny':
 
                         # There are conditions in the bucket policy that require
       analysis
                    if len(missingContextValues) > 0:
        
                      logger.push_event(key_name,'W09',missingContextValues)
  
                            issue_found = True
                    else:
     
                         logger.push_event(key_name,'W05')
                    
          if len(object_public_acl) == 0:
                            logger.push_event(key_name,'E06')

                                  issue_found = True
                          
        continue  # cut early, next object
                        if block_ignore_public_acl:

                                  logger.push_event(key_name,'E07')
           
                       issue_found = True
                            continue 
       # cut early, next object
                else: # evaluation == 'error'

                          logger.push_event(key_name,'E14',evaluation,missingContextValues)
          
            else: # No bucket policy attached
                if len(object_public_acl)
       == 0:
                    logger.push_event(key_name,'E18')
           
       
            
            object_metadata = _get_object_metadata(bucket,key_name)

                  if not object_metadata:
                logger.push_event(key_name,'E16')

                      issue_found = True
            if object_metadata.get('WebsiteRedirectLocation'):

                      logger.push_event(key_name,'I06')
            if object_metadata.get('DeleteMarker'):

                      logger.push_event(key_name,'W06')
            if object_metadata.get('ReplicationStatus'):

                      logger.push_event(key_name,'I13', object_metadata.get('ReplicationStatus'))

                  if object_metadata.get('ObjectLockLegalHoldStatus') == 'ON':

                      logger.push_event(key_name,'W07')
            if object_metadata.get('StorageClass'):

                      if object_metadata['StorageClass'] in ('GLACIER','DEEP_ARCHIVE'):

                          logger.push_event(key_name,'E08',object_metadata['StorageClass'])

                          issue_found = True
                    continue  # cut
       early, next object
                    
            sse = _check_encryption(bucket,key_name)

                  if sse == 'aws:kms':
                logger.push_event(key_name,'E09')

                      issue_found = True
                continue  # cut early,
       next object
            elif sse == 'error':
                logger.push_event(key_name,'E17')

                      issue_found = True
            else: # 'AES256'
        
              logger.push_event(key_name,'I07')
                
            if
       issue_found:
                logger.push_event(key_name,'E19')
        
          else:
                if ignore_block_public:
                    logger.push_event(key_name,'W08')

                      else:
                    logger.push_event(key_name,'I08')

                      
            continue # next object

        if obj_num
       == 0:
            logger.push_event(bucket,'E11',prefix)
            dump,
       summary, report = logger.report()
            return { 'error':True, 
 
                     'json':dump,
                'summary':summary,
         
             'report':report
                }              
        else:
  
                if issue_found:
                logger.push_event(bucket,'E12')

                  else:
                logger.push_event(bucket,'I11')

    
          logger.push_event(bucket,'I12')
        dump, summary, report = logger.report()

              return { 'error':issue_found, 
                'json': dump,
   
                   'summary': summary,
                'report':report
       
               }
    
    except Exception as e:
        _print_error(e,'object
       analysis','analysis')
        logger.push_event(bucket,'E13',str(e))
  
            raise
    
    finally:
        #Push events to CloudWatch
     
         logger.flush()
"
  outputs:
  - Name: "json"
    Selector: "$.Payload.json"
    Type: "String"
  - Name: "error"
    Selector: "$.Payload.error"
    Type: "Boolean"
  - Name: "bucket"
    Selector: "$.Payload.summary"
    Type: "String"
  - Name: "objects"
    Selector: "$.Payload.report"
    Type: "String"
outputs:
- "AnalyzeObjects.objects"
- "AnalyzeObjects.bucket"
